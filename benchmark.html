
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Benchmark &#8212; fibber 0.3.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="_static/dai-logo-white.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="fibber package" href="api/fibber.html" />
    <link rel="prev" title="Data Format" href="dataformat.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/dai-logo-white-200.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="readme.html">
  Fibber
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="dataformat.html">
  Data Format
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Benchmark
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="api/fibber.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="contributing.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="authors.html">
  Credits
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="history.html">
  History
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dai-lab/fibber" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#built-in-datasets">
   Built-in Datasets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmark-result">
   Benchmark result
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-usage">
   Basic Usage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparation">
     Preparation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-benchmark-as-a-module">
     Run benchmark as a module
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-in-a-python-script-jupyter-notebook">
     Run in a python script / jupyter notebook
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-overview-result">
     Generate overview result
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advanced-usage">
   Advanced Usage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#customize-dataset">
     Customize dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#customize-classifier">
     Customize classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#customize-strategy">
     Customize strategy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adversarial-training">
   Adversarial Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-and-test-using-command-line">
     Training and test using command line
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="benchmark">
<h1>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h1>
<p>Benchmark module is an important component in Fibber. It provides an easy-to-use
API and is highly customizable. In this document, we will show</p>
<ul class="simple">
<li><p>Built-in Datasets: we preprocessed 6 datasets into <a class="reference external" href="https://dai-lab.github.io/fibber/dataformat.html">fibber’s format</a>.</p></li>
<li><p>Benchmark result: we benchmark all built-in methods on built-in dataset.</p></li>
<li><p>Basic usage: how to use builtin strategies to attack BERT classifier on a built-in dataset.</p></li>
<li><p>Advance usage: how to customize strategy, classifier, and dataset.</p></li>
</ul>
<div class="section" id="built-in-datasets">
<h2>Built-in Datasets<a class="headerlink" href="#built-in-datasets" title="Permalink to this headline">¶</a></h2>
<p>Here is the information about datasets in fibber.</p>
<table class="table">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Size (train/test)</p></th>
<th class="head"><p>Classes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Topic Classification</p></td>
<td><p><a class="reference external" href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html">ag</a></p></td>
<td><p>120k / 7.6k</p></td>
<td><p>World / Sport / Business / Sci-Tech</p></td>
</tr>
<tr class="row-odd"><td><p>Sentiment classification</p></td>
<td><p><a class="reference external" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">mr</a></p></td>
<td><p>9k / 1k</p></td>
<td><p>Negative / Positive</p></td>
</tr>
<tr class="row-even"><td><p>Sentiment classification</p></td>
<td><p><a class="reference external" href="https://academictorrents.com/details/66ab083bda0c508de6c641baabb1ec17f72dc480">yelp</a></p></td>
<td><p>160k / 38k</p></td>
<td><p>Negative / Positive</p></td>
</tr>
<tr class="row-odd"><td><p>Sentiment classification</p></td>
<td><p><a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/">imdb</a></p></td>
<td><p>25k / 25k</p></td>
<td><p>Negative / Positive</p></td>
</tr>
<tr class="row-even"><td><p>Natural Language Inference</p></td>
<td><p><a class="reference external" href="https://nlp.stanford.edu/projects/snli/">snli</a></p></td>
<td><p>570k / 10k</p></td>
<td><p>Entailment / Neutral / Contradict</p></td>
</tr>
<tr class="row-odd"><td><p>Natural Language Inference</p></td>
<td><p><a class="reference external" href="https://cims.nyu.edu/~sbowman/multinli/">mnli</a></p></td>
<td><p>433k / 10k</p></td>
<td><p>Entailment / Neutral / Contradict</p></td>
</tr>
</tbody>
</table>
<p>Note that ag has two configurations. In <code class="docutils literal notranslate"><span class="pre">ag</span></code>, we combines the title and content as input for classification. In <code class="docutils literal notranslate"><span class="pre">ag_no_title</span></code>, we use only use content as input.</p>
<p>Note that mnli has two configurations. Use <code class="docutils literal notranslate"><span class="pre">mnli</span></code> for matched testset, and <code class="docutils literal notranslate"><span class="pre">mnli_mis</span></code> for mismatched testset.</p>
</div>
<div class="section" id="benchmark-result">
<h2>Benchmark result<a class="headerlink" href="#benchmark-result" title="Permalink to this headline">¶</a></h2>
<p>The following table shows the benchmarking result. (Here we show the number of wins.)</p>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Strategy Name</p></th>
<th class="head"><p>After Attack Accuracy</p></th>
<th class="head"><p>Cross Encoder Similarity</p></th>
<th class="head"><p>Perplexity Ratio</p></th>
<th class="head"><p>GloVe Similarity</p></th>
<th class="head"><p>USE Similarity</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>IdentityStrategy</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>TextFoolerJin2019</p></td>
<td><p>26</p></td>
<td><p>10</p></td>
<td><p>3</p></td>
<td><p>10</p></td>
<td><p>14</p></td>
</tr>
<tr class="row-even"><td><p>BERTAttackLi2020</p></td>
<td><p>18</p></td>
<td><p>13</p></td>
<td><p>18</p></td>
<td><p>22</p></td>
<td><p>21</p></td>
</tr>
<tr class="row-odd"><td><p>BAEGarg2019</p></td>
<td><p>11</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>PSOZang2020</p></td>
<td><p>9</p></td>
<td><p>9</p></td>
<td><p>11</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-odd"><td><p>ASRSStrategy</p></td>
<td><p>30</p></td>
<td><p>22</p></td>
<td><p>22</p></td>
<td><p>14</p></td>
<td><p>9</p></td>
</tr>
</tbody>
</table>
<p>For detailed tables, see <a class="reference external" href="https://docs.google.com/spreadsheets/d/1B_5RiMfndNVhxZLX5ykMqt5SCjpy3MxOovBi_RL41Fw/edit?usp=sharing">Google Sheet</a>.</p>
</div>
<div class="section" id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline">¶</a></h2>
<p>In this short tutorial, we will guide you through a series of steps that will help you run
benchmark on builtin strategies and datasets.</p>
<div class="section" id="preparation">
<h3>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">¶</a></h3>
<p><strong>Install Fibber:</strong> Please follow the instructions to <a class="reference external" href="https://dai-lab.github.io/fibber/readme.html#install">Install Fibber</a>.**</p>
<p><strong>Download datasets:</strong> Please use the following command to download all datasets.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m fibber.datasets.download_datasets
</pre></div>
</div>
<p>All datasets will be downloaded and stored at <code class="docutils literal notranslate"><span class="pre">~/.fibber/datasets</span></code>.</p>
</div>
<div class="section" id="run-benchmark-as-a-module">
<h3>Run benchmark as a module<a class="headerlink" href="#run-benchmark-as-a-module" title="Permalink to this headline">¶</a></h3>
<p>If you are trying to reproduce the performance table, running the benchmark as a module is
recommended.</p>
<p>The following command will run the <code class="docutils literal notranslate"><span class="pre">BertSamplingStrategy</span></code> strategy on the <code class="docutils literal notranslate"><span class="pre">mr</span></code> dataset. To use other
datasets, see the <a class="reference external" href="#Datasets">datasets</a> section.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m fibber.benchmark.benchmark <span class="se">\</span>
    --dataset mr <span class="se">\</span>
    --strategy ASRSStrategy <span class="se">\</span>
    --output_dir exp-mr <span class="se">\</span>
    --num_paraphrases_per_text <span class="m">20</span> <span class="se">\</span>
    --subsample_testset <span class="m">100</span> <span class="se">\</span>
    --gpt2_gpu <span class="m">0</span> <span class="se">\</span>
    --bert_gpu <span class="m">0</span> <span class="se">\</span>
    --use_gpu <span class="m">0</span> <span class="se">\</span>
    --ce_gpu_id<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
    --bert_clf_steps <span class="m">20000</span>
</pre></div>
</div>
<p>It first subsamples the test set to <code class="docutils literal notranslate"><span class="pre">100</span></code> examples, then generates <code class="docutils literal notranslate"><span class="pre">20</span></code> paraphrases for each
example. During this process, the paraphrased sentences will be stored at
<code class="docutils literal notranslate"><span class="pre">exp-mr/mr-BertSamplingStrategy-&lt;date&gt;-&lt;time&gt;-tmp.json</span></code>.</p>
<p>Then the pipeline will initialize all the evaluation metrics.</p>
<ul class="simple">
<li><p>We will use a <code class="docutils literal notranslate"><span class="pre">GPT2</span></code> model to evaluate if a sentence is meaningful. The <code class="docutils literal notranslate"><span class="pre">GPT2</span></code> language model will be executed on <code class="docutils literal notranslate"><span class="pre">gpt2_gpu</span></code>. You should change the argument to a proper GPU id.</p></li>
<li><p>We will use a <code class="docutils literal notranslate"><span class="pre">Universal</span> <span class="pre">sentence</span> <span class="pre">encoder</span> <span class="pre">(USE)</span></code> model to measure the similarity between two paraphrased sentences and the original sentence. The <code class="docutils literal notranslate"><span class="pre">USE</span></code> will be executed on <code class="docutils literal notranslate"><span class="pre">use_gpu</span></code>. You should change the argument to a proper GPU id.</p></li>
<li><p>We will use a <code class="docutils literal notranslate"><span class="pre">BERT</span></code> model to predict the classification label for paraphrases. The <code class="docutils literal notranslate"><span class="pre">BERT</span></code> will be executed on <code class="docutils literal notranslate"><span class="pre">bert_gpu</span></code>. You should change the argument to a proper GPU id. <strong>Note that the BERT classifier will be trained for the first time you execute the pipeline. Then the trained model will be saved at ``~/.fibber/bert_clf/&lt;dataset_name&gt;/``. Because of the training, it will use more GPU memory than GPT2 and USE. So assign BERT to a separate GPU if you have multiple GPUs.</strong></p></li>
</ul>
<p>After the execution, the evaluation metric for each of the paraphrases will be stored at <code class="docutils literal notranslate"><span class="pre">exp-ag/ag-RandomStrategy-&lt;date&gt;-&lt;time&gt;-with-metrics.json</span></code>.</p>
<p>The aggregated result will be stored as a row at <code class="docutils literal notranslate"><span class="pre">~/.fibber/results/detailed.csv</span></code>.</p>
</div>
<div class="section" id="run-in-a-python-script-jupyter-notebook">
<h3>Run in a python script / jupyter notebook<a class="headerlink" href="#run-in-a-python-script-jupyter-notebook" title="Permalink to this headline">¶</a></h3>
<p>You may want to integrate the benchmark framework into your own python script. We also provide easy to use APIs.</p>
<p><strong>Create a Benchmark object</strong> The following code will create a fibber Benchmark object on <code class="docutils literal notranslate"><span class="pre">mr</span></code> dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fibber.benchmark</span> <span class="kn">import</span> <span class="n">Benchmark</span>

<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;exp-debug&quot;</span><span class="p">,</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;mr&quot;</span><span class="p">,</span>
    <span class="n">subsample_attack_set</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">use_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">gpt2_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">ce_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_clf_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">bert_clf_bs</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Similarly, you can assign different components to different GPUs.</p>
<p><strong>Run benchmark</strong> Use the following code to run the benchmark using a specific strategy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark</span><span class="o">.</span><span class="n">run_benchmark</span><span class="p">(</span><span class="n">paraphrase_strategy</span><span class="o">=</span><span class="s2">&quot;BertSamplingStrategy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="generate-overview-result">
<h3>Generate overview result<a class="headerlink" href="#generate-overview-result" title="Permalink to this headline">¶</a></h3>
<p>We use the number of wins to compare different strategies. To generate the overview table, use the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m fibber.benchmark.make_overview
</pre></div>
</div>
<p>The overview table will be stored at <code class="docutils literal notranslate"><span class="pre">~/.fibber/results/overview.csv</span></code>.</p>
<p>Before running this command, please verify <code class="docutils literal notranslate"><span class="pre">~/.fibber/results/detailed.csv</span></code>. Each strategy must not have more than one executions on one dataset. Otherwise, the script will raise assertion errors.</p>
</div>
</div>
<div class="section" id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="customize-dataset">
<h3>Customize dataset<a class="headerlink" href="#customize-dataset" title="Permalink to this headline">¶</a></h3>
<p>To run a benchmark on a customized classification dataset, you should first convert a dataset into <a class="reference external" href="https://dai-lab.github.io/fibber/dataformat.html">fibber’s format</a>.</p>
<p>Then construct a benchmark object using your own dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;exp-debug&quot;</span><span class="p">,</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;customized_dataset&quot;</span><span class="p">,</span>

    <span class="c1">### Pass your processed datasets here. ####</span>
    <span class="n">trainset</span> <span class="o">=</span> <span class="n">your_train_set</span><span class="p">,</span>
    <span class="n">testset</span> <span class="o">=</span> <span class="n">your_test_set</span><span class="p">,</span>
    <span class="n">attack_set</span> <span class="o">=</span> <span class="n">your_attack_set</span><span class="p">,</span>
    <span class="c1">###########################################</span>

    <span class="n">subsample_attack_set</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">use_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">gpt2_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_clf_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">bert_clf_bs</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="customize-classifier">
<h3>Customize classifier<a class="headerlink" href="#customize-classifier" title="Permalink to this headline">¶</a></h3>
<p>To customize classifier, use the <code class="docutils literal notranslate"><span class="pre">customized_clf</span></code> arg in Benchmark. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># a naive classifier that always outputs 0.</span>
<span class="k">class</span> <span class="nc">CustomizedClf</span><span class="p">(</span><span class="n">ClassifierBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">measure_example</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">origin</span><span class="p">,</span> <span class="n">paraphrase</span><span class="p">,</span> <span class="n">data_record</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">paraphrase_field</span><span class="o">=</span><span class="s2">&quot;text0&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;exp-debug&quot;</span><span class="p">,</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;mr&quot;</span><span class="p">,</span>

    <span class="c1"># Pass your customized classifier here.</span>
    <span class="c1"># Note that the Benchmark class will NOT train the classifier.</span>
    <span class="c1"># So please train your classifier before pass it to Benchmark.</span>
    <span class="n">customized_clf</span><span class="o">=</span><span class="n">CustomizedClf</span><span class="p">(),</span>

    <span class="n">subsample_attack_set</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">use_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">gpt2_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">bert_clf_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">bert_clf_bs</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="customize-strategy">
<h3>Customize strategy<a class="headerlink" href="#customize-strategy" title="Permalink to this headline">¶</a></h3>
<p>To customize strategy, you should create a strategy object then call the <code class="docutils literal notranslate"><span class="pre">run_benchmark</span></code> function. For example,
we want to benchmark BertSamplingStrategy using a different set of hyper parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BertSamplingStrategy</span><span class="p">(</span>
    <span class="n">arg_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;bs_clf_weight&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;mr&quot;</span><span class="p">,</span>
    <span class="n">strategy_gpu_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;exp_mr&quot;</span><span class="p">,</span>
    <span class="n">metric_bundle</span><span class="o">=</span><span class="n">benchmark</span><span class="o">.</span><span class="n">get_metric_bundle</span><span class="p">())</span>

<span class="n">benchmark</span><span class="o">.</span><span class="n">run_benchmark</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="adversarial-training">
<h2>Adversarial Training<a class="headerlink" href="#adversarial-training" title="Permalink to this headline">¶</a></h2>
<p>Adversarial training is a natural way to defend against attacks.</p>
<p>Fibber provides a simple way to fine-tune a classifier on paraphrases generated by paraphrase strategies.</p>
<div class="section" id="training-and-test-using-command-line">
<h3>Training and test using command line<a class="headerlink" href="#training-and-test-using-command-line" title="Permalink to this headline">¶</a></h3>
<p>The following command uses bert sampling strategy to fine-tune the default bert classifier.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">fibber</span><span class="o">.</span><span class="n">benchmark</span><span class="o">.</span><span class="n">benchmark</span> \
    <span class="o">--</span><span class="n">robust_tuning</span> <span class="mi">1</span> \
    <span class="o">--</span><span class="n">robust_tuning_steps</span> <span class="mi">5000</span> \
    <span class="o">--</span><span class="n">dataset</span> <span class="n">mr</span> \
    <span class="o">--</span><span class="n">strategy</span> <span class="n">BertSamplingStrategy</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="n">exp</span><span class="o">-</span><span class="n">mr</span> \
    <span class="o">--</span><span class="n">num_paraphrases_per_text</span> <span class="mi">20</span> \
    <span class="o">--</span><span class="n">subsample_testset</span> <span class="mi">100</span> \
    <span class="o">--</span><span class="n">gpt2_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">bert_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">use_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">bert_clf_steps</span> <span class="mi">5000</span>
</pre></div>
</div>
<p>The fine-tuned classifier will be stored at <code class="docutils literal notranslate"><span class="pre">~/.fibber/bert_clf/mr/DefaultTuningStrategy-BertSamplingStrategy</span></code></p>
<p>After the fine-tuning, you can use the following command to attack the fine-tuned classifier using BertSamplingStrategy. You do not need to use the same paraphrasing strategy for tuning and attack.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">fibber</span><span class="o">.</span><span class="n">benchmark</span><span class="o">.</span><span class="n">benchmark</span> \
    <span class="o">--</span><span class="n">robust_tuning</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">robust_tuning_steps</span> <span class="mi">5000</span> \
    <span class="o">--</span><span class="n">load_robust_tuned_clf_desc</span> <span class="n">DefaultTuningStrategy</span><span class="o">-</span><span class="n">BertSamplingStrategy</span> \
    <span class="o">--</span><span class="n">dataset</span> <span class="n">mr</span> \
    <span class="o">--</span><span class="n">strategy</span> <span class="n">BertSamplingStrategy</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="n">exp</span><span class="o">-</span><span class="n">mr</span> \
    <span class="o">--</span><span class="n">num_paraphrases_per_text</span> <span class="mi">20</span> \
    <span class="o">--</span><span class="n">subsample_testset</span> <span class="mi">100</span> \
    <span class="o">--</span><span class="n">gpt2_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">bert_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">use_gpu</span> <span class="mi">0</span> \
    <span class="o">--</span><span class="n">bert_clf_steps</span> <span class="mi">5000</span>
</pre></div>
</div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="dataformat.html" title="previous page">Data Format</a>
    <a class='right-next' id="next-link" href="api/fibber.html" title="next page">fibber package</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, MIT Data To AI Lab.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.1.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>