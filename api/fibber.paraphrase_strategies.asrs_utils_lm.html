
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>fibber.paraphrase_strategies.asrs_utils_lm module &#8212; fibber 0.3.0 documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/dai-logo-white.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="fibber.paraphrase_strategies.asrs_utils_text_parser module" href="fibber.paraphrase_strategies.asrs_utils_text_parser.html" />
    <link rel="prev" title="fibber.paraphrase_strategies.asrs_strategy module" href="fibber.paraphrase_strategies.asrs_strategy.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/dai-logo-white-200.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../readme.html">
  Fibber
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../dataformat.html">
  Data Format
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../benchmark.html">
  Benchmark
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="fibber.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../contributing.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../authors.html">
  Credits
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../history.html">
  History
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dai-lab/fibber" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fibber.benchmark.html">
   fibber.benchmark package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.benchmark.benchmark.html">
     fibber.benchmark.benchmark module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.benchmark.benchmark_utils.html">
     fibber.benchmark.benchmark_utils module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.benchmark.make_overview.html">
     fibber.benchmark.make_overview module
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fibber.datasets.html">
   fibber.datasets package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.dataset_utils.html">
     fibber.datasets.dataset_utils module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.download_datasets.html">
     fibber.datasets.download_datasets module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.downloadable_datasets.html">
     fibber.datasets.downloadable_datasets module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_ag.html">
     fibber.datasets.preprocess_ag module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_imdb.html">
     fibber.datasets.preprocess_imdb module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_mnli.html">
     fibber.datasets.preprocess_mnli module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_mr.html">
     fibber.datasets.preprocess_mr module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_snli.html">
     fibber.datasets.preprocess_snli module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_utils.html">
     fibber.datasets.preprocess_utils module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.datasets.preprocess_yelp.html">
     fibber.datasets.preprocess_yelp module
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fibber.metrics.html">
   fibber.metrics package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.attack_aggregation_utils.html">
     fibber.metrics.attack_aggregation_utils module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.bert_classifier.html">
     fibber.metrics.bert_classifier module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.ce_semantic_similarity_metric.html">
     fibber.metrics.ce_semantic_similarity_metric module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.classifier_base.html">
     fibber.metrics.classifier_base module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.edit_distance_metric.html">
     fibber.metrics.edit_distance_metric module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.glove_semantic_similarity_metric.html">
     fibber.metrics.glove_semantic_similarity_metric module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.gpt2_grammar_quality_metric.html">
     fibber.metrics.gpt2_grammar_quality_metric module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.metric_base.html">
     fibber.metrics.metric_base module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.metric_utils.html">
     fibber.metrics.metric_utils module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.metrics.use_semantic_similarity_metric.html">
     fibber.metrics.use_semantic_similarity_metric module
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="fibber.paraphrase_strategies.html">
   fibber.paraphrase_strategies package
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.asrs_strategy.html">
     fibber.paraphrase_strategies.asrs_strategy module
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     fibber.paraphrase_strategies.asrs_utils_lm module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.asrs_utils_text_parser.html">
     fibber.paraphrase_strategies.asrs_utils_text_parser module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.asrs_utils_wpe.html">
     fibber.paraphrase_strategies.asrs_utils_wpe module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.identity_strategy.html">
     fibber.paraphrase_strategies.identity_strategy module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.narrl_strategy.html">
     fibber.paraphrase_strategies.narrl_strategy module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.non_autoregressive_bert_sampling_strategy.html">
     fibber.paraphrase_strategies.non_autoregressive_bert_sampling_strategy module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.random_strategy.html">
     fibber.paraphrase_strategies.random_strategy module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.strategy_base.html">
     fibber.paraphrase_strategies.strategy_base module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.paraphrase_strategies.textattack_strategy.html">
     fibber.paraphrase_strategies.textattack_strategy module
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fibber.resources.html">
   fibber.resources package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.resources.download_resources.html">
     fibber.resources.download_resources module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.resources.downloadable_resources.html">
     fibber.resources.downloadable_resources module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.resources.resource_utils.html">
     fibber.resources.resource_utils module
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fibber.robust_tuning_strategy.html">
   fibber.robust_tuning_strategy package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.robust_tuning_strategy.default_tuning_strategy.html">
     fibber.robust_tuning_strategy.default_tuning_strategy module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fibber.robust_tuning_strategy.tuning_strategy_base.html">
     fibber.robust_tuning_strategy.tuning_strategy_base module
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="fibber.download_utils.html">
   fibber.download_utils module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fibber.fibber.html">
   fibber.fibber module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fibber.log.html">
   fibber.log module
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="module-fibber.paraphrase_strategies.asrs_utils_lm">
<span id="fibber-paraphrase-strategies-asrs-utils-lm-module"></span><h1>fibber.paraphrase_strategies.asrs_utils_lm module<a class="headerlink" href="#module-fibber.paraphrase_strategies.asrs_utils_lm" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.Env">
<em class="property">class </em><code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">Env</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">seq</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">tok_type</span></em>, <em class="sig-param"><span class="n">raw_text</span></em>, <em class="sig-param"><span class="n">use_metric</span></em>, <em class="sig-param"><span class="n">tokenizer</span></em>, <em class="sig-param"><span class="n">max_step</span></em>, <em class="sig-param"><span class="n">max_mask_rate</span></em>, <em class="sig-param"><span class="n">lm_model</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#Env"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.Env" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.Env.get_sentence_emb">
<code class="sig-name descname">get_sentence_emb</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#Env.get_sentence_emb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.Env.get_sentence_emb" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.Env.get_state">
<code class="sig-name descname">get_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#Env.get_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.Env.get_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.Env.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">seq_candidate</span></em>, <em class="sig-param"><span class="n">logits_candidate</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#Env.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.Env.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM">
<em class="property">class </em><code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">NARRLBertLM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">sentence_embed_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#NARRLBertLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.bert.modeling_bert.BertForMaskedLM</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sentence_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">apply_cls</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#NARRLBertLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – Labels for computing the masked language modeling loss. Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span>
<span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring) Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored
(masked), the loss is only computed for the tokens with labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Masked language modeling (MLM) loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;The capital of France is [MASK].&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NARRLBertLM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM">
<em class="property">class </em><code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">NonAutoregressiveBertLM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">sentence_embed_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#NonAutoregressiveBertLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.bert.modeling_bert.BertForMaskedLM</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sentence_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">apply_cls</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#NonAutoregressiveBertLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – Labels for computing the masked language modeling loss. Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span>
<span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring) Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored
(masked), the loss is only computed for the tokens with labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Masked language modeling (MLM) loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedLMOutput</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;The capital of France is [MASK].&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.NonAutoregressiveBertLM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.compute_lm_loss">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">compute_lm_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lm_model</span></em>, <em class="sig-param"><span class="n">seq</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">tok_type</span></em>, <em class="sig-param"><span class="n">lm_label</span></em>, <em class="sig-param"><span class="n">stats</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#compute_lm_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.compute_lm_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute masked language model training loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm_model</strong> (<em>transformers.BertForMaskedLM</em>) – a BERT language model.</p></li>
<li><p><strong>seq</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the word
pieces.</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the attention
mask.</p></li>
<li><p><strong>tok_type</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the
token type id.</p></li>
<li><p><strong>lm_label</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the label
for each position. Use -100 if the loss is not computed for that position.</p></li>
<li><p><strong>stats</strong> (<em>dist</em>) – a dictionary storing training stats.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Scalar) a scalar loss value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.compute_non_autoregressive_lm_loss">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">compute_non_autoregressive_lm_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lm_model</span></em>, <em class="sig-param"><span class="n">sentence_embeds</span></em>, <em class="sig-param"><span class="n">seq</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">tok_type</span></em>, <em class="sig-param"><span class="n">lm_label</span></em>, <em class="sig-param"><span class="n">stats</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#compute_non_autoregressive_lm_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.compute_non_autoregressive_lm_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute masked language model training loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm_model</strong> (<em>transformers.BertForMaskedLM</em>) – a BERT language model.</p></li>
<li><p><strong>sentence_embeds</strong> (<em>np.array</em>) – an numpy array of sentence embeddings.</p></li>
<li><p><strong>seq</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the word
pieces.</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the attention
mask.</p></li>
<li><p><strong>tok_type</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the
token type id.</p></li>
<li><p><strong>lm_label</strong> (<em>torch.Tensor</em>) – an int tensor of size (batch_size, length) representing the label
for each position. Use -100 if the loss is not computed for that position.</p></li>
<li><p><strong>stats</strong> (<em>dist</em>) – a dictionary storing training stats.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Scalar) a scalar loss value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.fine_tune_lm">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">fine_tune_lm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_dir</span></em>, <em class="sig-param"><span class="n">trainset</span></em>, <em class="sig-param"><span class="n">filter</span></em>, <em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">lm_steps</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">lm_bs</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">lm_opt</span><span class="o">=</span><span class="default_value">'adamw'</span></em>, <em class="sig-param"><span class="n">lm_lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lm_decay</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">lm_period_summary</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">lm_period_save</span><span class="o">=</span><span class="default_value">5000</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#fine_tune_lm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.fine_tune_lm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a finetuned BERT language model on a given dataset.</p>
<p>The language model will be stored at <code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_all</span></code> if filter is -1, or
<code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_filter_?</span></code> if filter is not -1.</p>
<p>If filter is not -1. The pretrained langauge model will first be pretrained on the while
dataset, then it will be finetuned on the data excluding the filter category.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_dir</strong> (<em>str</em>) – a directory to store pretrained language model.</p></li>
<li><p><strong>trainset</strong> (<a class="reference internal" href="fibber.datasets.html#fibber.datasets.DatasetForBert" title="fibber.datasets.DatasetForBert"><em>DatasetForBert</em></a>) – the training set for finetune the language model.</p></li>
<li><p><strong>filter</strong> (<em>int</em>) – a category to exclude from finetuning.</p></li>
<li><p><strong>device</strong> (<em>torch.Device</em>) – a device to train the model.</p></li>
<li><p><strong>lm_steps</strong> (<em>int</em>) – finetuning steps.</p></li>
<li><p><strong>lm_bs</strong> (<em>int</em>) – finetuning batch size.</p></li>
<li><p><strong>lm_opt</strong> (<em>str</em>) – optimzer name. choose from [“sgd”, “adam”, “adamW”].</p></li>
<li><p><strong>lm_lr</strong> (<em>float</em>) – learning rate.</p></li>
<li><p><strong>lm_decay</strong> (<em>float</em>) – weight decay for the optimizer.</p></li>
<li><p><strong>lm_period_summary</strong> (<em>int</em>) – number of steps to write training summary.</p></li>
<li><p><strong>lm_period_save</strong> (<em>int</em>) – number of steps to save the finetuned model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a finetuned language model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(BertForMaskedLM)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.get_lm">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">get_lm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lm_option</span></em>, <em class="sig-param"><span class="n">output_dir</span></em>, <em class="sig-param"><span class="n">trainset</span></em>, <em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">lm_steps</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">lm_bs</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">lm_opt</span><span class="o">=</span><span class="default_value">'adamw'</span></em>, <em class="sig-param"><span class="n">lm_lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lm_decay</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">lm_period_summary</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">lm_period_save</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#get_lm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.get_lm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a BERT language model or a list of language models on a given dataset.</p>
<p>The language model will be stored at <code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_all</span></code> if lm_option is finetune.
The language model will be stored at <code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_filter_?</span></code> if lm_option is adv.</p>
<p>If filter is not -1. The pretrained language model will first be pretrained on the while
dataset, then it will be finetuned on the data excluding the filter category.</p>
<p>The re</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm_option</strong> (<em>str</em>) – choose from <cite>[“pretrain”, “finetune”, “adv”, “nartune”]</cite>.
pretrain means the pretrained BERT model without fine-tuning on current
dataset.
finetune means fine-tuning the BERT model on current dataset.
adv means adversarial tuning on current dataset.
nartune means tuning the</p></li>
<li><p><strong>output_dir</strong> (<em>str</em>) – a directory to store pretrained language model.</p></li>
<li><p><strong>trainset</strong> (<a class="reference internal" href="fibber.datasets.html#fibber.datasets.DatasetForBert" title="fibber.datasets.DatasetForBert"><em>DatasetForBert</em></a>) – the training set for finetune the language model.</p></li>
<li><p><strong>device</strong> (<em>torch.Device</em>) – a device to train the model.</p></li>
<li><p><strong>lm_steps</strong> (<em>int</em>) – finetuning steps.</p></li>
<li><p><strong>lm_bs</strong> (<em>int</em>) – finetuning batch size.</p></li>
<li><p><strong>lm_opt</strong> (<em>str</em>) – optimzer name. choose from [“sgd”, “adam”, “adamW”].</p></li>
<li><p><strong>lm_lr</strong> (<em>float</em>) – learning rate.</p></li>
<li><p><strong>lm_decay</strong> (<em>float</em>) – weight decay for the optimizer.</p></li>
<li><p><strong>lm_period_summary</strong> (<em>int</em>) – number of steps to write training summary.</p></li>
<li><p><strong>lm_period_save</strong> (<em>int</em>) – number of steps to save the finetuned model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>the tokenizer for the language model.
(BertForMaskedLM): a finetuned language model if lm_option is pretrain or finetune.
([BertForMaskedLM]): a list of finetuned language model if lm_option is adv. The i-th</p>
<blockquote>
<div><p>language model in the list is fine-tuned on data not having label i.</p>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(BertTokenizerFast)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.new_stats">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">new_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#new_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.new_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new stats dict.</p>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.non_autoregressive_fine_tune_lm">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">non_autoregressive_fine_tune_lm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_dir</span></em>, <em class="sig-param"><span class="n">trainset</span></em>, <em class="sig-param"><span class="n">filter</span></em>, <em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">use_metric</span></em>, <em class="sig-param"><span class="n">model_class</span></em>, <em class="sig-param"><span class="n">num_keywords</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">split_sentences</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">lm_steps</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">lm_bs</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">lm_opt</span><span class="o">=</span><span class="default_value">'adamw'</span></em>, <em class="sig-param"><span class="n">lm_lr</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">lm_decay</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">lm_period_summary</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">lm_period_save</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">lm_pretune_steps</span><span class="o">=</span><span class="default_value">20000</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#non_autoregressive_fine_tune_lm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.non_autoregressive_fine_tune_lm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a finetuned BERT language model on a given dataset.</p>
<p>The language model will be stored at <code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_all</span></code> if filter is -1, or
<code class="docutils literal notranslate"><span class="pre">&lt;output_dir&gt;/lm_filter_?</span></code> if filter is not -1.</p>
<p>If filter is not -1. The pretrained langauge model will first be pretrained on the while
dataset, then it will be finetuned on the data excluding the filter category.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_dir</strong> (<em>str</em>) – a directory to store pretrained language model.</p></li>
<li><p><strong>trainset</strong> (<a class="reference internal" href="fibber.datasets.html#fibber.datasets.DatasetForBert" title="fibber.datasets.DatasetForBert"><em>DatasetForBert</em></a>) – the training set for finetune the language model.</p></li>
<li><p><strong>filter</strong> (<em>int</em>) – a category to exclude from finetuning.</p></li>
<li><p><strong>device</strong> (<em>torch.Device</em>) – a device to train the model.</p></li>
<li><p><strong>use_metric</strong> (<a class="reference internal" href="fibber.metrics.html#fibber.metrics.USESemanticSimilarityMetric" title="fibber.metrics.USESemanticSimilarityMetric"><em>USESemanticSimilarityMetric</em></a>) – a sentence encoder metric</p></li>
<li><p><strong>model_class</strong> (<em>class</em>) – choose from <code class="docutils literal notranslate"><span class="pre">[NonAutoregressiveBertLM,</span> <span class="pre">NARRLBertLM]</span></code></p></li>
<li><p><strong>split_sentences</strong> (<em>bool</em>) – whether to train a sentence level language model.</p></li>
<li><p><strong>lm_steps</strong> (<em>int</em>) – finetuning steps.</p></li>
<li><p><strong>lm_bs</strong> (<em>int</em>) – finetuning batch size.</p></li>
<li><p><strong>lm_opt</strong> (<em>str</em>) – optimzer name. choose from [“sgd”, “adam”, “adamW”].</p></li>
<li><p><strong>lm_lr</strong> (<em>float</em>) – learning rate.</p></li>
<li><p><strong>lm_decay</strong> (<em>float</em>) – weight decay for the optimizer.</p></li>
<li><p><strong>lm_period_summary</strong> (<em>int</em>) – number of steps to write training summary.</p></li>
<li><p><strong>lm_period_save</strong> (<em>int</em>) – number of steps to save the finetuned model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a finetuned language model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(BertForMaskedLM)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.rl_fine_tune_lm">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">rl_fine_tune_lm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_dir</span></em>, <em class="sig-param"><span class="n">trainset</span></em>, <em class="sig-param"><span class="n">filter</span></em>, <em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">lm_model</span></em>, <em class="sig-param"><span class="n">use_metric</span></em>, <em class="sig-param"><span class="n">num_keywords</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">split_sentences</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">num_decode_iter</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">max_mask_rate</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">discount</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">rl_steps</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="n">rl_bs</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">lm_opt</span><span class="o">=</span><span class="default_value">'adamw'</span></em>, <em class="sig-param"><span class="n">lm_lr</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">lm_decay</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">lm_period_summary</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">lm_period_save</span><span class="o">=</span><span class="default_value">5000</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#rl_fine_tune_lm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.rl_fine_tune_lm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="fibber.paraphrase_strategies.asrs_utils_lm.write_summary">
<code class="sig-prename descclassname">fibber.paraphrase_strategies.asrs_utils_lm.</code><code class="sig-name descname">write_summary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stats</span></em>, <em class="sig-param"><span class="n">summary</span></em>, <em class="sig-param"><span class="n">global_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fibber/paraphrase_strategies/asrs_utils_lm.html#write_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#fibber.paraphrase_strategies.asrs_utils_lm.write_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save langauge model training summary.</p>
</dd></dl>

</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="fibber.paraphrase_strategies.asrs_strategy.html" title="previous page">fibber.paraphrase_strategies.asrs_strategy module</a>
    <a class='right-next' id="next-link" href="fibber.paraphrase_strategies.asrs_utils_text_parser.html" title="next page">fibber.paraphrase_strategies.asrs_utils_text_parser module</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, MIT Data To AI Lab.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.1.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>